---
###########################
# Common
###########################
compatibility_version: 4.2

# Data center
data_center_name: Default

##########################
# VM infra
##########################
template_cluster: "{{ rhv_cluster }}"
template_name: centos7
template_memory: 8GiB
template_cpu: 1
template_disk_storage: "{{ rhv_data_storage }}"
template_disk_size: 60GiB
template_nics:
  - name: nic1
    profile_name: ovirtmgmt
    interface: virtio

##########################
# Other top scope vars
##########################
debug_vm_create: true

master_vm:
  cluster: "{{ rhv_cluster }}"
  template: "{{ template_name }}"
  memory: 16GiB
  cores: 2
  high_availability: true
  disks:
    - size: 100GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 50GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio

node_vm:
  cluster: "{{ rhv_cluster }}"
  template: "{{ template_name }}"
  memory: 8GiB
  cores: 2
  disks:
    - size: 100GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: docker_disk
      interface: virtio
    - size: 50GiB
      storage_domain: "{{ rhv_data_storage }}"
      name: localvol_disk
      interface: virtio


##########################
# Cloud Init Script
##########################
# Use the following if RHEL 7.4 or below VMs are being created on a RHV 4.2 or above engine

update_guest_agent_script: |
  runcmd:
    - sed -i 's@^# device =.*@device = /dev/virtio-ports/ovirt-guest-agent.0@' /etc/ovirt-guest-agent.conf
    - sed -i 's@com.redhat.rhevm.vdsm@ovirt-guest-agent.0@' /etc/udev/rules.d/55-ovirt-guest-agent.rules
    - 'udevadm trigger --subsystem-match="virtio-ports"'
    - chmod 777 /dev/virtio-ports/ovirt-guest-agent.0
    - systemctl restart ovirt-guest-agent

vms:
  # Master VMs
  - name: "master0.{{ public_hosted_zone }}"
    profile: "{{ master_vm }}"
    tag: openshift_master
    cloud_init:
      host_name: "master0.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "master1.{{ public_hosted_zone }}"
    tag: openshift_master
    profile: "{{ master_vm }}"
    cloud_init:
      host_name: "master1.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "master2.{{ public_hosted_zone }}"
    tag: openshift_master
    profile: "{{ master_vm }}"
    cloud_init:
      host_name: "master2.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"

  # Infra VMs
  - name: "infra0.{{ public_hosted_zone }}"
    tag: openshift_infra
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "infra0.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "infra1.{{ public_hosted_zone }}"
    tag: openshift_infra
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "infra1.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "infra2.{{ public_hosted_zone }}"
    tag: openshift_infra
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "infra2.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"

  # Node VMs
  - name: "app0.{{ public_hosted_zone }}"
    tag: openshift_node
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "app0.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "app1.{{ public_hosted_zone }}"
    tag: openshift_node
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "app1.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"
  - name: "app2.{{ public_hosted_zone }}"
    tag: openshift_node
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "app2.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"

  # Load balancer
  - name: "lb.{{ public_hosted_zone }}"
    tag: openshift_lb
    profile: "{{ node_vm }}"
    cloud_init:
      host_name: "lb.{{ public_hosted_zone }}"
      authorized_ssh_keys: "{{ root_ssh_key }}"
      custom_script: "{{ update_guest_agent_script }}"

affinity_groups:
  - name: masters_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - "master0.{{ public_hosted_zone }}"
      - "master1.{{ public_hosted_zone }}"
      - "master2.{{ public_hosted_zone }}"
    wait: true
  - name: infra_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - "infra0.{{ public_hosted_zone }}"
      - "infra1.{{ public_hosted_zone }}"
      - "infra2.{{ public_hosted_zone }}"
    wait: true

  - name: app_ag
    cluster: "{{ rhv_cluster }}"
    vm_enforcing: false
    vm_rule: negative
    vms:
      - "app0.{{ public_hosted_zone }}"
      - "app1.{{ public_hosted_zone }}"
      - "app2.{{ public_hosted_zone }}"
...
